{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9e1c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4779e579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Skip to main content Epicurious 23 Best Nonalcoholic Drinks (Just Don’t Call Them Mocktails, Please) Recipes & Menus Expert Advice Ingredients Holidays & Events My Saved Recipes Sign In SUBSCRIBE Search Recipes & Menus Expert Advice Ingredients Holidays & Events Video My Saved Recipes Come see what's cooking. Over 50,000 recipes, straight from our test kitchen to yours. Unlimited access for only $40 $30 a year. SUBSCRIBE Non-Alcoholic 23 Best Nonalcoholic Drinks (Just Don’t Call Them Mocktails, \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"data/data.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    "\n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    "\n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fdb21b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140, 4, 276, 277, 25, 8, 35, 21, 36, 31, 141, 50, 51, 91, 142]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token=1)\n",
    "tokenizer.fit_on_texts([data])\n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('data/token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d722d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "PREDICT_INP=2 # number of words taken as input to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9afab133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  1955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[140,   4, 276, 277],\n",
       "       [  4, 276, 277,  25],\n",
       "       [276, 277,  25,   8],\n",
       "       [277,  25,   8,  35],\n",
       "       [ 25,   8,  35,  21],\n",
       "       [  8,  35,  21,  36],\n",
       "       [ 35,  21,  36,  31],\n",
       "       [ 21,  36,  31, 141],\n",
       "       [ 36,  31, 141,  50],\n",
       "       [ 31, 141,  50,  51]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(PREDICT_INP, len(sequence_data)):\n",
    "    words = sequence_data[i-PREDICT_INP:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ecf8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:PREDICT_INP])\n",
    "    y.append(i[PREDICT_INP])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e1a6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 22:44:41.401093: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-30 22:44:41.419243: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-30 22:44:41.463231: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=PREDICT_INP))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1988b87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 22:44:55.233872: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-07-30 22:44:55.355681: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1800000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "31/31 [==============================] - 13s 229ms/step - loss: 6.5893\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.48060, saving model to next_words.h5\n",
      "Epoch 2/70\n",
      "31/31 [==============================] - 7s 222ms/step - loss: 6.1258\n",
      "\n",
      "Epoch 00002: loss improved from 6.48060 to 6.12251, saving model to next_words.h5\n",
      "Epoch 3/70\n",
      "31/31 [==============================] - 8s 243ms/step - loss: 6.0229\n",
      "\n",
      "Epoch 00003: loss improved from 6.12251 to 6.00289, saving model to next_words.h5\n",
      "Epoch 4/70\n",
      "31/31 [==============================] - 8s 262ms/step - loss: 5.8908\n",
      "\n",
      "Epoch 00004: loss improved from 6.00289 to 5.90031, saving model to next_words.h5\n",
      "Epoch 5/70\n",
      "31/31 [==============================] - 9s 296ms/step - loss: 5.8048\n",
      "\n",
      "Epoch 00005: loss improved from 5.90031 to 5.80876, saving model to next_words.h5\n",
      "Epoch 6/70\n",
      "31/31 [==============================] - 10s 315ms/step - loss: 5.5749\n",
      "\n",
      "Epoch 00006: loss improved from 5.80876 to 5.66902, saving model to next_words.h5\n",
      "Epoch 7/70\n",
      "31/31 [==============================] - 8s 257ms/step - loss: 5.5531\n",
      "\n",
      "Epoch 00007: loss improved from 5.66902 to 5.54195, saving model to next_words.h5\n",
      "Epoch 8/70\n",
      "31/31 [==============================] - 8s 257ms/step - loss: 5.3434\n",
      "\n",
      "Epoch 00008: loss improved from 5.54195 to 5.36306, saving model to next_words.h5\n",
      "Epoch 9/70\n",
      "31/31 [==============================] - 8s 252ms/step - loss: 5.2237\n",
      "\n",
      "Epoch 00009: loss improved from 5.36306 to 5.15348, saving model to next_words.h5\n",
      "Epoch 10/70\n",
      "31/31 [==============================] - 8s 251ms/step - loss: 4.9271\n",
      "\n",
      "Epoch 00010: loss improved from 5.15348 to 4.92876, saving model to next_words.h5\n",
      "Epoch 11/70\n",
      "31/31 [==============================] - 8s 247ms/step - loss: 4.6429\n",
      "\n",
      "Epoch 00011: loss improved from 4.92876 to 4.68965, saving model to next_words.h5\n",
      "Epoch 12/70\n",
      "31/31 [==============================] - 8s 252ms/step - loss: 4.4095\n",
      "\n",
      "Epoch 00012: loss improved from 4.68965 to 4.45688, saving model to next_words.h5\n",
      "Epoch 13/70\n",
      "31/31 [==============================] - 8s 252ms/step - loss: 4.2493\n",
      "\n",
      "Epoch 00013: loss improved from 4.45688 to 4.28774, saving model to next_words.h5\n",
      "Epoch 14/70\n",
      "31/31 [==============================] - 8s 253ms/step - loss: 4.0199\n",
      "\n",
      "Epoch 00014: loss improved from 4.28774 to 4.06705, saving model to next_words.h5\n",
      "Epoch 15/70\n",
      "31/31 [==============================] - 8s 268ms/step - loss: 3.8097\n",
      "\n",
      "Epoch 00015: loss improved from 4.06705 to 3.91772, saving model to next_words.h5\n",
      "Epoch 16/70\n",
      "31/31 [==============================] - 8s 262ms/step - loss: 3.6248\n",
      "\n",
      "Epoch 00016: loss improved from 3.91772 to 3.68358, saving model to next_words.h5\n",
      "Epoch 17/70\n",
      "31/31 [==============================] - 8s 245ms/step - loss: 3.2723\n",
      "\n",
      "Epoch 00017: loss improved from 3.68358 to 3.41725, saving model to next_words.h5\n",
      "Epoch 18/70\n",
      "31/31 [==============================] - 8s 264ms/step - loss: 3.0886\n",
      "\n",
      "Epoch 00018: loss improved from 3.41725 to 3.18932, saving model to next_words.h5\n",
      "Epoch 19/70\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 2.8462\n",
      "\n",
      "Epoch 00019: loss improved from 3.18932 to 3.03774, saving model to next_words.h5\n",
      "Epoch 20/70\n",
      "31/31 [==============================] - 9s 277ms/step - loss: 2.7291\n",
      "\n",
      "Epoch 00020: loss improved from 3.03774 to 2.83782, saving model to next_words.h5\n",
      "Epoch 21/70\n",
      "31/31 [==============================] - 8s 276ms/step - loss: 2.4833\n",
      "\n",
      "Epoch 00021: loss improved from 2.83782 to 2.62654, saving model to next_words.h5\n",
      "Epoch 22/70\n",
      "31/31 [==============================] - 8s 252ms/step - loss: 2.3338\n",
      "\n",
      "Epoch 00022: loss improved from 2.62654 to 2.49899, saving model to next_words.h5\n",
      "Epoch 23/70\n",
      "31/31 [==============================] - 8s 272ms/step - loss: 2.2206\n",
      "\n",
      "Epoch 00023: loss improved from 2.49899 to 2.28578, saving model to next_words.h5\n",
      "Epoch 24/70\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 1.9297\n",
      "\n",
      "Epoch 00024: loss improved from 2.28578 to 2.13411, saving model to next_words.h5\n",
      "Epoch 25/70\n",
      "31/31 [==============================] - 9s 279ms/step - loss: 1.9188\n",
      "\n",
      "Epoch 00025: loss improved from 2.13411 to 2.05506, saving model to next_words.h5\n",
      "Epoch 26/70\n",
      "31/31 [==============================] - 8s 274ms/step - loss: 1.7979\n",
      "\n",
      "Epoch 00026: loss improved from 2.05506 to 1.89967, saving model to next_words.h5\n",
      "Epoch 27/70\n",
      "31/31 [==============================] - 8s 260ms/step - loss: 1.5516\n",
      "\n",
      "Epoch 00027: loss improved from 1.89967 to 1.69722, saving model to next_words.h5\n",
      "Epoch 28/70\n",
      "31/31 [==============================] - 8s 265ms/step - loss: 1.5108\n",
      "\n",
      "Epoch 00028: loss improved from 1.69722 to 1.61757, saving model to next_words.h5\n",
      "Epoch 29/70\n",
      "31/31 [==============================] - 8s 274ms/step - loss: 1.3833\n",
      "\n",
      "Epoch 00029: loss improved from 1.61757 to 1.49910, saving model to next_words.h5\n",
      "Epoch 30/70\n",
      "31/31 [==============================] - 8s 265ms/step - loss: 1.2209\n",
      "\n",
      "Epoch 00030: loss improved from 1.49910 to 1.36329, saving model to next_words.h5\n",
      "Epoch 31/70\n",
      "31/31 [==============================] - 8s 268ms/step - loss: 1.1701\n",
      "\n",
      "Epoch 00031: loss improved from 1.36329 to 1.30317, saving model to next_words.h5\n",
      "Epoch 32/70\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 1.0148\n",
      "\n",
      "Epoch 00032: loss improved from 1.30317 to 1.16134, saving model to next_words.h5\n",
      "Epoch 33/70\n",
      "31/31 [==============================] - 8s 268ms/step - loss: 0.9948\n",
      "\n",
      "Epoch 00033: loss improved from 1.16134 to 1.13351, saving model to next_words.h5\n",
      "Epoch 34/70\n",
      "31/31 [==============================] - 8s 264ms/step - loss: 0.9014\n",
      "\n",
      "Epoch 00034: loss improved from 1.13351 to 0.98579, saving model to next_words.h5\n",
      "Epoch 35/70\n",
      "31/31 [==============================] - 9s 280ms/step - loss: 0.7977\n",
      "\n",
      "Epoch 00035: loss improved from 0.98579 to 0.87559, saving model to next_words.h5\n",
      "Epoch 36/70\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 0.7230\n",
      "\n",
      "Epoch 00036: loss improved from 0.87559 to 0.81931, saving model to next_words.h5\n",
      "Epoch 37/70\n",
      "31/31 [==============================] - 9s 278ms/step - loss: 0.6764\n",
      "\n",
      "Epoch 00037: loss improved from 0.81931 to 0.76647, saving model to next_words.h5\n",
      "Epoch 38/70\n",
      "31/31 [==============================] - 9s 299ms/step - loss: 0.6494\n",
      "\n",
      "Epoch 00038: loss improved from 0.76647 to 0.74332, saving model to next_words.h5\n",
      "Epoch 39/70\n",
      "31/31 [==============================] - 9s 281ms/step - loss: 0.5838\n",
      "\n",
      "Epoch 00039: loss improved from 0.74332 to 0.62611, saving model to next_words.h5\n",
      "Epoch 40/70\n",
      "31/31 [==============================] - 8s 258ms/step - loss: 0.5320\n",
      "\n",
      "Epoch 00040: loss improved from 0.62611 to 0.58394, saving model to next_words.h5\n",
      "Epoch 41/70\n",
      "31/31 [==============================] - 8s 266ms/step - loss: 0.3912\n",
      "\n",
      "Epoch 00041: loss improved from 0.58394 to 0.47941, saving model to next_words.h5\n",
      "Epoch 42/70\n",
      "31/31 [==============================] - 8s 257ms/step - loss: 0.4476\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.47941\n",
      "Epoch 43/70\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 0.4828\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.47941\n",
      "Epoch 44/70\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 0.4584\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.47941\n",
      "Epoch 45/70\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 0.4284\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.47941\n",
      "Epoch 46/70\n",
      "31/31 [==============================] - 8s 266ms/step - loss: 0.4046\n",
      "\n",
      "Epoch 00046: loss improved from 0.47941 to 0.44594, saving model to next_words.h5\n",
      "Epoch 47/70\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.3382\n",
      "\n",
      "Epoch 00047: loss improved from 0.44594 to 0.37354, saving model to next_words.h5\n",
      "Epoch 48/70\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 0.3099\n",
      "\n",
      "Epoch 00048: loss improved from 0.37354 to 0.37105, saving model to next_words.h5\n",
      "Epoch 49/70\n",
      "31/31 [==============================] - 8s 261ms/step - loss: 0.3093\n",
      "\n",
      "Epoch 00049: loss improved from 0.37105 to 0.35694, saving model to next_words.h5\n",
      "Epoch 50/70\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 0.3006\n",
      "\n",
      "Epoch 00050: loss improved from 0.35694 to 0.30349, saving model to next_words.h5\n",
      "Epoch 51/70\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.2527\n",
      "\n",
      "Epoch 00051: loss improved from 0.30349 to 0.28194, saving model to next_words.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/70\n",
      "31/31 [==============================] - 8s 257ms/step - loss: 0.2022\n",
      "\n",
      "Epoch 00052: loss improved from 0.28194 to 0.24290, saving model to next_words.h5\n",
      "Epoch 53/70\n",
      "31/31 [==============================] - 8s 252ms/step - loss: 0.2345\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.24290\n",
      "Epoch 54/70\n",
      "31/31 [==============================] - 9s 277ms/step - loss: 0.2165\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.24290\n",
      "Epoch 55/70\n",
      "31/31 [==============================] - 8s 267ms/step - loss: 0.2321\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.24290\n",
      "Epoch 56/70\n",
      "31/31 [==============================] - 8s 259ms/step - loss: 0.2177\n",
      "\n",
      "Epoch 00056: loss improved from 0.24290 to 0.22717, saving model to next_words.h5\n",
      "Epoch 57/70\n",
      "31/31 [==============================] - 8s 267ms/step - loss: 0.1707\n",
      "\n",
      "Epoch 00057: loss improved from 0.22717 to 0.19933, saving model to next_words.h5\n",
      "Epoch 58/70\n",
      "31/31 [==============================] - 8s 262ms/step - loss: 0.2312\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.19933\n",
      "Epoch 59/70\n",
      "31/31 [==============================] - 9s 294ms/step - loss: 0.2002\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.19933\n",
      "Epoch 60/70\n",
      "31/31 [==============================] - 8s 268ms/step - loss: 0.1956\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.19933\n",
      "Epoch 61/70\n",
      "31/31 [==============================] - 8s 265ms/step - loss: 0.1605\n",
      "\n",
      "Epoch 00061: loss improved from 0.19933 to 0.18953, saving model to next_words.h5\n",
      "Epoch 62/70\n",
      "31/31 [==============================] - 8s 275ms/step - loss: 0.1815\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.18953\n",
      "Epoch 63/70\n",
      "31/31 [==============================] - 8s 267ms/step - loss: 0.1735\n",
      "\n",
      "Epoch 00063: loss improved from 0.18953 to 0.17533, saving model to next_words.h5\n",
      "Epoch 64/70\n",
      "31/31 [==============================] - 8s 265ms/step - loss: 0.1700\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.17533\n",
      "Epoch 65/70\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 0.1778\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.17533\n",
      "Epoch 66/70\n",
      "31/31 [==============================] - 8s 264ms/step - loss: 0.2528\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.17533\n",
      "Epoch 67/70\n",
      "31/31 [==============================] - 9s 277ms/step - loss: 0.3197\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.17533\n",
      "Epoch 68/70\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 0.4283\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.17533\n",
      "Epoch 69/70\n",
      "31/31 [==============================] - 8s 257ms/step - loss: 0.4159\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.17533\n",
      "Epoch 70/70\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 0.3922\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.17533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcdbc752e80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"data/next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
